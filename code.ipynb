{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT GET ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 per time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取文件到列表A\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "# 2. 将列表A每五个句子为一个单位抽取出来形成一个新的列表B\n",
    "step = 50\n",
    "nls_zhData = [ls_zhData[i:i + step] for i in range(0, len(ls_zhData), step)]\n",
    "# nls_zhData=nls_zhData[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置你的OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP3:\n",
      "12/24 | 13/24 | 14/24 | 15/24 | 16/24 | 17/24 | 18/24 | 19/24 | 20/24 | 21/24 | 22/24 | 23/24 | 24/24 | "
     ]
    }
   ],
   "source": [
    "for tpi in range(2,len(TPs)):\n",
    "    print('\\nTP'+str(tpi+1)+':')\n",
    "    tp = TPs[tpi]\n",
    "    # with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "    #     f.write(\"%s\\n\" % (tp))\n",
    "    # 4. 循环询问GPT\n",
    "    lsAns = []  # 用于存储GPT的回答\n",
    "    for nlsi in range(11, len(nls_zhData)):\n",
    "        unit = nls_zhData[nlsi]\n",
    "        askContent = tp+\"\".join(unit)\n",
    "        # print(askContent)\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            { \n",
    "            \"role\": \"user\",\n",
    "            \"content\": askContent\n",
    "            } \n",
    "    ]\n",
    "    )\n",
    "        ans = completion.choices[0].message[\"content\"]\n",
    "        lsAns.append(ans)\n",
    "        # print(ans)\n",
    "        # print('\\n'+'-'*10+'\\n')\n",
    "\n",
    "        # 5. 保存C到文件\n",
    "        with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "            f.write(\"-[%d-%d]-\\n%s\\n\" % (nlsi*50,nlsi*50+50,ans))\n",
    "        print(\"%d/%d\"%(nlsi+1,len(nls_zhData)),end=' | ')\n",
    "        time.sleep(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate these sentences from Chinese to English:\\n发明的是一个伟大的科学家。\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test openai.ChatCompletion.create\n",
    "import openai\n",
    "import time\n",
    "\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "\n",
    "sample = ls_zhData[0]\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]\n",
    "\n",
    "askContent = TPs[0]+\"\".join(sample)\n",
    "askContent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x2895b236090> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"4\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\",\"content\": \"pls say %d\"%(i)} for i in range(5)]\n",
    ")\n",
    "completion.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x2895b236090> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"4\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"role\": \"user\",\"content\": \"pls say %d\"%(i)} for i in range(5)]\n",
    "completion['choices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x2895b085d60> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"The invention was made by a great scientist.\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ans = completion.choices\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 / 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP1:\n",
      "Translate these sentences from Chinese to English:\n",
      "敲打的是鼓。\n",
      "\n",
      "21/1200 | The drum is being beaten.\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test openai.ChatCompletion.create\n",
    "import openai\n",
    "import time\n",
    "\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]\n",
    "\n",
    "# for tpi in range(0,len(TPs)):\n",
    "for tpi in range(0,1):\n",
    "    print('\\nTP'+str(tpi+1)+':')\n",
    "    tp = TPs[tpi]\n",
    "    with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "        f.write(\"%s\\n\" % (tp))\n",
    "    # 4. 循环询问GPT\n",
    "    lsAns = []  # 用于存储GPT的回答\n",
    "    for nlsi in range(20, 21):\n",
    "        unit = ls_zhData[nlsi]\n",
    "        askContent = tp+\"\".join(unit)\n",
    "        print(askContent)\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            { \n",
    "            \"role\": \"user\",\n",
    "            \"content\": askContent\n",
    "            } \n",
    "    ]\n",
    "    )\n",
    "        ans = completion.choices[0].message[\"content\"]\n",
    "        lsAns.append(ans)\n",
    "        print(\"%d/%d\"%(nlsi+1,len(ls_zhData)),end=' | ')\n",
    "        print(ans)\n",
    "        print('\\n'+'-'*10+'\\n')\n",
    "\n",
    "        # 5. 保存C到文件\n",
    "        with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "            f.write(\"%s\\n\" % (ans))\n",
    "        time.sleep(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK LINE NO. OF DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 初始化三个文件列表\n",
    "file_lists = [[], [], []]\n",
    "\n",
    "# 读取文件到列表中\n",
    "for i in range(1, 4):\n",
    "    with open(f\"tp{i}_translated_sentences.txt\", 'r', encoding='utf-8') as f:\n",
    "        file_lists[i-1] = f.readlines()\n",
    "\n",
    "# 对每个文件列表进行处理\n",
    "for i, file_list in enumerate(file_lists, start=1):\n",
    "    with open(f\"file{i}.out\", 'w', encoding='utf-8') as f:\n",
    "        print(f\"处理文件 tp{i}_translated_sentences.txt\", file=f)\n",
    "        for line_no, line in enumerate(file_list, start=1):\n",
    "            match = re.search(r'-\\[(.*?)\\]-\\n', line)\n",
    "            if match:\n",
    "                print(f\"在第 {line_no} 行找到匹配项: {match.group()}\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "ref_file = 'dataset/test.true.en'\n",
    "tslt_file = 'translated/tp1_tslt_en.txt'\n",
    "\n",
    "refs = read_file(ref_file)\n",
    "tslts = read_file(tslt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# TEST SAMPLE\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# 注意：在references中，每个参考句子都是一个列表的元素，即使只有一个参考句子\n",
    "references = [\n",
    "                [['this', 'is', 'test', 'one'], ['this', 'is', 'a', 'test', 'one']],\n",
    "                [['this', 'is', 'test', 'two'], ['this', 'is', 'a', 'test', 'two']]\n",
    "             ]\n",
    "candidates = [\n",
    "                ['this', 'is', 'test', 'one'],\n",
    "                ['this', 'is', 'test', 'two']\n",
    "             ]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29861622524500175\n"
     ]
    }
   ],
   "source": [
    "# DATA VIEW\n",
    "\n",
    "# tokenize \n",
    "refs_tok = [[wordpunct_tokenize(sentence)] for sentence in refs]\n",
    "tslts_tok = [wordpunct_tokenize(sentence) for sentence in tslts]\n",
    "\n",
    "score = corpus_bleu(refs_tok, tslts_tok)\n",
    "print(\"BLEU\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8806, 0.8840, 1.0000, 0.8951]) tensor([0.8807, 0.8846, 1.0000, 0.8951]) tensor([0.8807, 0.8843, 1.0000, 0.8951])\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "cands = ['这是候选句子1', '这是候选句子2', '这是候选句子3','正确的东西']\n",
    "refs = ['这是参考句子1', '这是参考句子2', '这是候选句子3','错误的东西']\n",
    "P, R, F1 = score(cands, refs, lang='en', model_type='bert-base-chinese')\n",
    "print(P, R, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:   1%|          | 21.0M/3.04G [00:23<11:59, 4.19MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\Rockindics\\Data\\JianGuoYun\\我的坚果云\\学习笔记盒_Rockindics\\Note\\2308\\nlp2ct_cr_project\\code.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/nlp2ct_cr_project/code.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m P, R, F1 \u001b[39m=\u001b[39m score(tslts, refs, lang\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m, model_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmicrosoft/deberta-xlarge-mnli\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/nlp2ct_cr_project/code.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m avg_bert_score_P_R_F1\u001b[39m=\u001b[39m[P\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem(), R\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem(), F1\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem]\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/nlp2ct_cr_project/code.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m avg_bert_score_P_R_F1\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\bert_score\\score.py:98\u001b[0m, in \u001b[0;36mscore\u001b[1;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[0;32m     95\u001b[0m     num_layers \u001b[39m=\u001b[39m model2layers[model_type]\n\u001b[0;32m     97\u001b[0m tokenizer \u001b[39m=\u001b[39m get_tokenizer(model_type, use_fast_tokenizer)\n\u001b[1;32m---> 98\u001b[0m model \u001b[39m=\u001b[39m get_model(model_type, num_layers, all_layers)\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\bert_score\\utils.py:255\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(model_type, num_layers, all_layers)\u001b[0m\n\u001b[0;32m    253\u001b[0m     model \u001b[39m=\u001b[39m T5EncoderModel\u001b[39m.\u001b[39mfrom_pretrained(model_type)\n\u001b[0;32m    254\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 255\u001b[0m     model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_type)\n\u001b[0;32m    256\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mdecoder\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m )\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\transformers\\modeling_utils.py:2929\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2926\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2927\u001b[0m         \u001b[39m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m         filename \u001b[39m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 2929\u001b[0m         resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2930\u001b[0m             pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs\n\u001b[0;32m   2931\u001b[0m         )\n\u001b[0;32m   2932\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   2933\u001b[0m     \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   2934\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2935\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2936\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   2937\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   2938\u001b[0m     )\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\transformers\\utils\\hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    430\u001b[0m         path_or_repo_id,\n\u001b[0;32m    431\u001b[0m         filename,\n\u001b[0;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1431\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1431\u001b[0m     http_get(\n\u001b[0;32m   1432\u001b[0m         url_to_download,\n\u001b[0;32m   1433\u001b[0m         temp_file,\n\u001b[0;32m   1434\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1435\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1436\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1437\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1438\u001b[0m     )\n\u001b[0;32m   1440\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1441\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:551\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    541\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    544\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    545\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    550\u001b[0m )\n\u001b[1;32m--> 551\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    553\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\urllib3\\response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 936\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    938\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    939\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[0;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[1;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[0;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[0;32m    883\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\urllib3\\response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\urllib3\\response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\http\\client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\http\\client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "P, R, F1 = score(tslts, refs, lang='en', model_type='microsoft/deberta-xlarge-mnli')\n",
    "avg_bert_score_P_R_F1=[P.mean().item(), R.mean().item(), F1.mean().item]\n",
    "print(\"avg_bert_score[P, R, F1]:\")\n",
    "avg_bert_score_P_R_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bleurt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7463241815567017\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from bleurt import score\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "checkpoint = \"bleurt/test_checkpoint\"\n",
    "references = [\"This is a test.\"]\n",
    "candidates = [\"This is the test.\"]\n",
    "\n",
    "scorer = score.BleurtScorer()\n",
    "scores = scorer.score(references=references, candidates=candidates)\n",
    "# assert isinstance(scores, list) and len(scores) == 1\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_scores = [scorer.score(references=[r], candidates=[t]) for r,t in zip(refs, tslts)]\n",
    "ls_scores = [i[0] for i in ls_scores]\n",
    "avg_bleurt_score = sum(ls_scores)/len(ls_scores)\n",
    "print(\"avg_bleurt_score:\")\n",
    "avg_bleurt_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev4symProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

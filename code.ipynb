{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取文件到列表A\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "# 2. 将列表A每五个句子为一个单位抽取出来形成一个新的列表B\n",
    "step = 50\n",
    "nls_zhData = [ls_zhData[i:i + step] for i in range(0, len(ls_zhData), step)]\n",
    "# nls_zhData=nls_zhData[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置你的OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP3:\n",
      "12/24 | 13/24 | 14/24 | 15/24 | 16/24 | 17/24 | 18/24 | 19/24 | 20/24 | 21/24 | 22/24 | 23/24 | 24/24 | "
     ]
    }
   ],
   "source": [
    "for tpi in range(2,len(TPs)):\n",
    "    print('\\nTP'+str(tpi+1)+':')\n",
    "    tp = TPs[tpi]\n",
    "    # with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "    #     f.write(\"%s\\n\" % (tp))\n",
    "    # 4. 循环询问GPT\n",
    "    lsAns = []  # 用于存储GPT的回答\n",
    "    for nlsi in range(11, len(nls_zhData)):\n",
    "        unit = nls_zhData[nlsi]\n",
    "        askContent = tp+\"\".join(unit)\n",
    "        # print(askContent)\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            { \n",
    "            \"role\": \"user\",\n",
    "            \"content\": askContent\n",
    "            } \n",
    "    ]\n",
    "    )\n",
    "        ans = completion.choices[0].message[\"content\"]\n",
    "        lsAns.append(ans)\n",
    "        # print(ans)\n",
    "        # print('\\n'+'-'*10+'\\n')\n",
    "\n",
    "        # 5. 保存C到文件\n",
    "        with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "            f.write(\"-[%d-%d]-\\n%s\\n\" % (nlsi*50,nlsi*50+50,ans))\n",
    "        print(\"%d/%d\"%(nlsi+1,len(nls_zhData)),end=' | ')\n",
    "        time.sleep(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 初始化三个文件列表\n",
    "file_lists = [[], [], []]\n",
    "\n",
    "# 读取文件到列表中\n",
    "for i in range(1, 4):\n",
    "    with open(f\"tp{i}_translated_sentences.txt\", 'r', encoding='utf-8') as f:\n",
    "        file_lists[i-1] = f.readlines()\n",
    "\n",
    "# 对每个文件列表进行处理\n",
    "for i, file_list in enumerate(file_lists, start=1):\n",
    "    with open(f\"file{i}.out\", 'w', encoding='utf-8') as f:\n",
    "        print(f\"处理文件 tp{i}_translated_sentences.txt\", file=f)\n",
    "        for line_no, line in enumerate(file_list, start=1):\n",
    "            match = re.search(r'-\\[(.*?)\\]-\\n', line)\n",
    "            if match:\n",
    "                print(f\"在第 {line_no} 行找到匹配项: {match.group()}\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 2.114660296685431e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 1.42G/1.42G [04:57<00:00, 4.78MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\Rockindics\\Data\\JianGuoYun\\我的坚果云\\学习笔记盒_Rockindics\\Note\\2308\\SeniorSYMproject\\code.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBLEU: \u001b[39m\u001b[39m{\u001b[39;00mbleu\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# # BLEURT\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# bleurt_scorer = bleurt_score.BleurtScorer('D:/Model/BLEURT-20')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# bleurt_scores = bleurt_scorer.score(references=refs, candidates=tslts)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# BERTScore\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m P, R, F1 \u001b[39m=\u001b[39m berts_score(refs, tslts, lang\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m avg_berts \u001b[39m=\u001b[39m F1\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Rockindics/Data/JianGuoYun/%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9C%E4%BA%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%9B%92_Rockindics/Note/2308/SeniorSYMproject/code.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAverage BERTScore: \u001b[39m\u001b[39m{\u001b[39;00mavg_berts\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4symProject\\lib\\site-packages\\bert_score\\score.py:129\u001b[0m, in \u001b[0;36mscore\u001b[1;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcalculating scores...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m--> 129\u001b[0m all_preds \u001b[39m=\u001b[39m bert_cos_score_idf(\n\u001b[0;32m    130\u001b[0m     model,\n\u001b[0;32m    131\u001b[0m     refs,\n\u001b[0;32m    132\u001b[0m     cands,\n\u001b[0;32m    133\u001b[0m     tokenizer,\n\u001b[0;32m    134\u001b[0m     idf_dict,\n\u001b[0;32m    135\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    136\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    137\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    138\u001b[0m     all_layers\u001b[39m=\u001b[39;49mall_layers,\n\u001b[0;32m    139\u001b[0m )\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m ref_group_boundaries \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     max_preds \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4symProject\\lib\\site-packages\\bert_score\\utils.py:489\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[1;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdedup_and_sort\u001b[39m(l):\n\u001b[0;32m    487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(l)), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)), reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 489\u001b[0m sentences \u001b[39m=\u001b[39m dedup_and_sort(refs \u001b[39m+\u001b[39;49m hyps)\n\u001b[0;32m    490\u001b[0m embs \u001b[39m=\u001b[39m []\n\u001b[0;32m    491\u001b[0m iter_range \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size)\n",
      "File \u001b[1;32md:\\Program_Files_IDE\\anaconda3\\envs\\ev4symProject\\lib\\site-packages\\bert_score\\utils.py:487\u001b[0m, in \u001b[0;36mbert_cos_score_idf.<locals>.dedup_and_sort\u001b[1;34m(l)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdedup_and_sort\u001b[39m(l):\n\u001b[1;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39;49m(l)), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)), reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\n",
    "from bert_score import score as berts_score\n",
    "from bleurt import score as bleurt_score\n",
    "\n",
    "# 读取文件\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "ref_file = 'dataset/test.true.en'\n",
    "tslt_file = 'translated/tp1_tslt_en.txt'\n",
    "\n",
    "refs = read_file(ref_file)\n",
    "tslts = read_file(tslt_file)\n",
    "\n",
    "# 实际评估时，应将refs和tslts转换为列表的列表，其中每个内层列表都是一个文档的句子\n",
    "refs = [[ref] for ref in refs]  # BLEU expects a list of references for each translation\n",
    "tslts = [tslt.split() for tslt in tslts]  # split each translation into a list of words\n",
    "\n",
    "# BLEU\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu = corpus_bleu(refs, tslts, smoothing_function=smoothie)\n",
    "print(f'BLEU: {bleu}')\n",
    "\n",
    "# # BLEURT\n",
    "# bleurt_scorer = bleurt_score.BleurtScorer('D:/Model/BLEURT-20')\n",
    "# bleurt_scores = bleurt_scorer.score(references=refs, candidates=tslts)\n",
    "# avg_bleurt = sum(bleurt_scores) / len(bleurt_scores)\n",
    "# print(f'Average BLEURT: {avg_bleurt}')\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = berts_score(refs, tslts, lang='en', verbose=True)\n",
    "avg_berts = F1.mean().item()\n",
    "print(f'Average BERTScore: {avg_berts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1200,), dtype=string, numpy=\n",
       "array([b'A great scientist invented it.', b'A robot was invented.',\n",
       "       b'High scores was cared about.', ...,\n",
       "       b\"The captain's mission today is to fly a large aircraft.\",\n",
       "       b'He was poor in the past, but now he is wealthy.',\n",
       "       b'This house is very wide.'], dtype=object)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from bleurt import score\n",
    "\n",
    "ref_file = 'dataset/test.true.en'\n",
    "tslt_file = 'translated/tp1_tslt_en.txt'\n",
    "\n",
    "# 读取参考翻译和机器翻译\n",
    "with open(ref_file, 'r', encoding='utf-8') as f:\n",
    "    reference_translations = f.read().splitlines()\n",
    "with open(tslt_file, 'r', encoding='utf-8') as f:\n",
    "    machine_translations = f.read().splitlines()\n",
    "\n",
    "# # 转换为 TensorFlow 常量\n",
    "references = tf.constant(reference_translations)\n",
    "candidates = tf.constant(machine_translations)\n",
    "\n",
    "references\n",
    "\n",
    "# # 创建 BLEURT 操作\n",
    "# checkpoint = 'D:/Model/BLEURT-20'  # 指定你的检查点路径\n",
    "# bleurt_ops = score.create_bleurt_ops(checkpoint)\n",
    "\n",
    "# # 计算 BLEURT 分数\n",
    "# bleurt_out = bleurt_ops(references=references, candidates=candidates)\n",
    "\n",
    "# # 打印 BLEURT 分数\n",
    "# print(bleurt_out[\"predictions\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev4symProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

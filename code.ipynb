{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-3.5 Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 spt(sentences per time), divided code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取文件到列表A\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "# 2. 将列表A每五个句子为一个单位抽取出来形成一个新的列表B\n",
    "step = 50\n",
    "nls_zhData = [ls_zhData[i:i + step] for i in range(0, len(ls_zhData), step)]\n",
    "# nls_zhData=nls_zhData[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置你的OpenAI API key\n",
    "openai.api_key = \n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP3:\n",
      "12/24 | 13/24 | 14/24 | 15/24 | 16/24 | 17/24 | 18/24 | 19/24 | 20/24 | 21/24 | 22/24 | 23/24 | 24/24 | "
     ]
    }
   ],
   "source": [
    "for tpi in range(2,len(TPs)):\n",
    "    print('\\nTP'+str(tpi+1)+':')\n",
    "    tp = TPs[tpi]\n",
    "    # with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "    #     f.write(\"%s\\n\" % (tp))\n",
    "    # 4. 循环询问GPT\n",
    "    lsAns = []  # 用于存储GPT的回答\n",
    "    for nlsi in range(11, len(nls_zhData)):\n",
    "        unit = nls_zhData[nlsi]\n",
    "        askContent = tp+\"\".join(unit)\n",
    "        # print(askContent)\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            { \n",
    "            \"role\": \"user\",\n",
    "            \"content\": askContent\n",
    "            } \n",
    "    ]\n",
    "    )\n",
    "        ans = completion.choices[0].message[\"content\"]\n",
    "        lsAns.append(ans)\n",
    "        # print(ans)\n",
    "        # print('\\n'+'-'*10+'\\n')\n",
    "\n",
    "        # 5. 保存C到文件\n",
    "        with open('tp'+str(tpi+1)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "            f.write(\"-[%d-%d]-\\n%s\\n\" % (nlsi*50,nlsi*50+50,ans))\n",
    "        print(\"%d/%d\"%(nlsi+1,len(nls_zhData)),end=' | ')\n",
    "        time.sleep(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate these sentences from Chinese to English:\\n发明的是一个伟大的科学家。\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test openai.ChatCompletion.create\n",
    "import openai\n",
    "import time\n",
    "\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "\n",
    "sample = ls_zhData[0]\n",
    "\n",
    "openai.api_key = \n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]\n",
    "\n",
    "askContent = TPs[0]+\"\".join(sample)\n",
    "askContent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x2895b236090> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"4\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\",\"content\": \"pls say %d\"%(i)} for i in range(5)]\n",
    ")\n",
    "completion.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x2895b236090> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"4\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"role\": \"user\",\"content\": \"pls say %d\"%(i)} for i in range(5)]\n",
    "completion['choices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x2895b085d60> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"The invention was made by a great scientist.\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ans = completion.choices\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK LINE NO. OF Translation results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 初始化三个文件列表\n",
    "file_lists = [[], [], []]\n",
    "\n",
    "# 读取文件到列表中\n",
    "for i in range(1, 4):\n",
    "    with open(f\"tp{i}_translated_sentences.txt\", 'r', encoding='utf-8') as f:\n",
    "        file_lists[i-1] = f.readlines()\n",
    "\n",
    "# 对每个文件列表进行处理\n",
    "for i, file_list in enumerate(file_lists, start=1):\n",
    "    with open(f\"file{i}.out\", 'w', encoding='utf-8') as f:\n",
    "        print(f\"处理文件 tp{i}_translated_sentences.txt\", file=f)\n",
    "        for line_no, line in enumerate(file_list, start=1):\n",
    "            match = re.search(r'-\\[(.*?)\\]-\\n', line)\n",
    "            if match:\n",
    "                print(f\"在第 {line_no} 行找到匹配项: {match.group()}\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 spt, get all translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP3:Please provide the English translation for these sentences:\n",
      "\n",
      "849/1200 |850/1200 |\n",
      "851/1200 |852/1200 |853/1200 |854/1200 |855/1200 |856/1200 |Error[3,857]: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "858/1200 |859/1200 |860/1200 |\n",
      "861/1200 |862/1200 |863/1200 |864/1200 |865/1200 |866/1200 |867/1200 |868/1200 |869/1200 |870/1200 |\n",
      "871/1200 |872/1200 |873/1200 |874/1200 |875/1200 |876/1200 |877/1200 |878/1200 |879/1200 |880/1200 |\n",
      "881/1200 |882/1200 |883/1200 |884/1200 |885/1200 |886/1200 |887/1200 |888/1200 |889/1200 |890/1200 |\n",
      "891/1200 |892/1200 |893/1200 |894/1200 |895/1200 |896/1200 |897/1200 |898/1200 |899/1200 |900/1200 |\n",
      "901/1200 |902/1200 |903/1200 |904/1200 |905/1200 |906/1200 |907/1200 |908/1200 |909/1200 |910/1200 |\n",
      "911/1200 |912/1200 |913/1200 |914/1200 |915/1200 |916/1200 |917/1200 |918/1200 |919/1200 |920/1200 |\n",
      "921/1200 |922/1200 |923/1200 |924/1200 |925/1200 |926/1200 |927/1200 |928/1200 |929/1200 |930/1200 |\n",
      "931/1200 |932/1200 |933/1200 |934/1200 |935/1200 |936/1200 |937/1200 |938/1200 |939/1200 |940/1200 |\n",
      "941/1200 |942/1200 |943/1200 |944/1200 |945/1200 |946/1200 |947/1200 |948/1200 |949/1200 |950/1200 |\n",
      "951/1200 |952/1200 |953/1200 |954/1200 |955/1200 |956/1200 |957/1200 |958/1200 |959/1200 |960/1200 |\n",
      "961/1200 |962/1200 |963/1200 |964/1200 |965/1200 |966/1200 |967/1200 |968/1200 |969/1200 |970/1200 |\n",
      "971/1200 |972/1200 |973/1200 |974/1200 |975/1200 |976/1200 |977/1200 |978/1200 |979/1200 |980/1200 |\n",
      "981/1200 |982/1200 |983/1200 |984/1200 |985/1200 |986/1200 |987/1200 |988/1200 |Error[3,989]: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "990/1200 |\n",
      "991/1200 |992/1200 |993/1200 |994/1200 |995/1200 |996/1200 |997/1200 |998/1200 |999/1200 |1000/1200 |\n",
      "1001/1200 |1002/1200 |1003/1200 |1004/1200 |1005/1200 |1006/1200 |1007/1200 |1008/1200 |1009/1200 |1010/1200 |\n",
      "1011/1200 |1012/1200 |1013/1200 |1014/1200 |1015/1200 |1016/1200 |1017/1200 |1018/1200 |1019/1200 |1020/1200 |\n",
      "1021/1200 |1022/1200 |1023/1200 |1024/1200 |1025/1200 |1026/1200 |1027/1200 |1028/1200 |1029/1200 |1030/1200 |\n",
      "1031/1200 |1032/1200 |1033/1200 |1034/1200 |1035/1200 |1036/1200 |1037/1200 |1038/1200 |1039/1200 |1040/1200 |\n",
      "1041/1200 |1042/1200 |1043/1200 |1044/1200 |1045/1200 |1046/1200 |1047/1200 |1048/1200 |1049/1200 |1050/1200 |\n",
      "1051/1200 |1052/1200 |1053/1200 |1054/1200 |1055/1200 |1056/1200 |1057/1200 |1058/1200 |1059/1200 |1060/1200 |\n",
      "1061/1200 |1062/1200 |1063/1200 |1064/1200 |1065/1200 |1066/1200 |1067/1200 |1068/1200 |1069/1200 |1070/1200 |\n",
      "1071/1200 |1072/1200 |1073/1200 |1074/1200 |1075/1200 |1076/1200 |1077/1200 |1078/1200 |1079/1200 |Error[3,1080]: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "1081/1200 |1082/1200 |1083/1200 |1084/1200 |1085/1200 |1086/1200 |1087/1200 |1088/1200 |1089/1200 |1090/1200 |\n",
      "1091/1200 |1092/1200 |1093/1200 |1094/1200 |1095/1200 |1096/1200 |1097/1200 |1098/1200 |1099/1200 |1100/1200 |\n",
      "1101/1200 |1102/1200 |1103/1200 |1104/1200 |1105/1200 |1106/1200 |1107/1200 |1108/1200 |1109/1200 |1110/1200 |\n",
      "1111/1200 |1112/1200 |1113/1200 |1114/1200 |1115/1200 |1116/1200 |1117/1200 |1118/1200 |1119/1200 |1120/1200 |\n",
      "1121/1200 |1122/1200 |1123/1200 |1124/1200 |1125/1200 |1126/1200 |1127/1200 |1128/1200 |1129/1200 |1130/1200 |\n",
      "1131/1200 |1132/1200 |Error[3,1133]: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Error[3,1134]: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "1135/1200 |1136/1200 |1137/1200 |1138/1200 |1139/1200 |1140/1200 |\n",
      "1141/1200 |1142/1200 |1143/1200 |1144/1200 |1145/1200 |1146/1200 |1147/1200 |1148/1200 |1149/1200 |1150/1200 |\n",
      "1151/1200 |1152/1200 |1153/1200 |1154/1200 |1155/1200 |1156/1200 |1157/1200 |1158/1200 |1159/1200 |1160/1200 |\n",
      "1161/1200 |1162/1200 |1163/1200 |1164/1200 |1165/1200 |1166/1200 |1167/1200 |1168/1200 |1169/1200 |1170/1200 |\n",
      "1171/1200 |1172/1200 |1173/1200 |1174/1200 |1175/1200 |1176/1200 |1177/1200 |1178/1200 |1179/1200 |1180/1200 |\n",
      "1181/1200 |1182/1200 |1183/1200 |1184/1200 |1185/1200 |1186/1200 |1187/1200 |1188/1200 |1189/1200 |1190/1200 |\n",
      "1191/1200 |1192/1200 |1193/1200 |1194/1200 |1195/1200 |1196/1200 |1197/1200 |Error[3,1198]: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "1199/1200 |1200/1200 |\n"
     ]
    }
   ],
   "source": [
    "# test openai.ChatCompletion.create\n",
    "import openai\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]\n",
    "\n",
    "\n",
    "start_tpi = -1\n",
    "start_nlsi = -1\n",
    "# 获取当前文件夹中的所有文件\n",
    "files = os.listdir('.')\n",
    "# 筛选出所有满足条件的文件，获取其中的数字部分，并转化为整数\n",
    "nums = [int(re.search(r'tp(\\d+)_translated_sentences\\.txt', f).group(1)) for f in files if re.match(r'tp(\\d+)_translated_sentences\\.txt', f)]\n",
    "# 获取最大的数字，如果没有满足条件的文件则返回None\n",
    "start_tpi = max(nums) if nums else None\n",
    "# 如果存在满足条件的文件\n",
    "if start_tpi is not None:\n",
    "    # 读取对应文件的倒数第二行\n",
    "    with open(f'tp{start_tpi}_translated_sentences.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) >= 2:\n",
    "            s = re.search(r'\\[(.*?)\\]', lines[-1])\n",
    "            s = s.group(1) if s else None\n",
    "            start_nlsi = int(s)+1\n",
    "\n",
    "tpi = 1\n",
    "while tpi <= len(TPs):\n",
    "    if start_tpi != -1:\n",
    "        tpi = start_tpi\n",
    "        start_tpi = -1\n",
    "    tp = TPs[tpi - 1]  # adjust index here\n",
    "    print('\\nTP'+str(tpi)+':'+tp)\n",
    "    # if start_tpi == -1:\n",
    "    #     with open('tp'+str(tpi)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "    #         f.write(\"TP%d: %s\\n\" % (tpi, tp))\n",
    "    # 4. 循环询问GPT\n",
    "    lsAns = []  # 用于存储GPT的回答\n",
    "    nlsi = 1\n",
    "    while nlsi <= len(ls_zhData):\n",
    "        if start_nlsi != -1:\n",
    "            nlsi = start_nlsi\n",
    "            start_nlsi = -1\n",
    "        unit = ls_zhData[nlsi - 1]  # adjust index here\n",
    "        askContent = tp + \"\".join(unit)\n",
    "        # print(askContent)\n",
    "        # if True:\n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                { \n",
    "                \"role\": \"user\",\n",
    "                \"content\": askContent\n",
    "                } \n",
    "        ]\n",
    "        )\n",
    "            ans = completion.choices[0].message[\"content\"]\n",
    "            lsAns.append(ans)\n",
    "            print(\"%d/%d\" % (nlsi, len(ls_zhData)), end=' |')\n",
    "            if nlsi % 10 == 0:\n",
    "                print()\n",
    "        except Exception as e:\n",
    "            ans = \"Error[%d,%d]: %s\" % (tpi, nlsi, str(e))\n",
    "            print(ans)\n",
    "            start_tpi = tpi\n",
    "            start_nlsi = nlsi + 1\n",
    "        # print(ans)\n",
    "        # print('\\n'+'-'*10+'\\n')\n",
    "    \n",
    "        # 5. 保存C到文件\n",
    "        with open('tp'+str(tpi)+'_translated_sentences.txt', 'a+', encoding='utf-8') as f:\n",
    "            f.write(\"[%d]:%s\\n\" % (nlsi, ans))\n",
    "        # time.sleep(0.02)\n",
    "        nlsi += 1\n",
    "    tpi += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SINGLE OUTPUT for fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP3:Please provide the English translation for these sentences:\n",
      "\n",
      "Please provide the English translation for these sentences:\n",
      "这句话是引用古书里的。\n",
      "\n",
      "1001/1200 |This sentence is a quotation from an ancient book.\n"
     ]
    }
   ],
   "source": [
    "# test openai.ChatCompletion.create\n",
    "import openai\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "with open('dataset/zh.source', 'r', encoding='utf-8') as file:\n",
    "    ls_zhData = file.readlines()\n",
    "\n",
    "\n",
    "openai.api_key = \n",
    "\n",
    "TPs = [\n",
    "'Translate these sentences from Chinese to English:\\n',  \n",
    "'Answer with no quotes. What do these sentences mean in English?\\n', \n",
    "'Please provide the English translation for these sentences:\\n'\n",
    "]\n",
    "\n",
    "\n",
    "tpi = 3\n",
    "nlsi = 1001\n",
    "if True:\n",
    "    tp = TPs[tpi - 1]  # adjust index here\n",
    "    print('\\nTP'+str(tpi)+':'+tp)\n",
    "    lsAns = []  # 用于存储GPT的回答\n",
    "    if True:\n",
    "        unit = ls_zhData[nlsi - 1]  # adjust index here\n",
    "        askContent = tp + \"\".join(unit)\n",
    "        print(askContent)\n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                { \n",
    "                \"role\": \"user\",\n",
    "                \"content\": askContent\n",
    "                } \n",
    "        ]\n",
    "        )\n",
    "            ans = completion.choices[0].message[\"content\"]\n",
    "            lsAns.append(ans)\n",
    "            print(\"%d/%d\" % (nlsi, len(ls_zhData)), end=' |')\n",
    "        except Exception as e:\n",
    "            ans = \"Error[%d,%d]: %s\" % (tpi, nlsi, str(e))\n",
    "            print(ans)\n",
    "            start_tpi = tpi\n",
    "            start_nlsi = nlsi + 1\n",
    "        print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION DIVIDED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "ref_file = 'dataset/test.true.en'\n",
    "tslt_file = 'translated/tp1_tslt_en.txt'\n",
    "\n",
    "refs = read_file(ref_file)\n",
    "tslts = read_file(tslt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# corpus_bleu SAMPLE\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# 注意：在references中，每个参考句子都是一个列表的元素，即使只有一个参考句子\n",
    "references = [\n",
    "                [['this', 'is', 'test', 'one'], ['this', 'is', 'a', 'test', 'one']],\n",
    "                [['this', 'is', 'test', 'two'], ['this', 'is', 'a', 'test', 'two']]\n",
    "             ]\n",
    "candidates = [\n",
    "                ['this', 'is', 'test', 'one'],\n",
    "                ['this', 'is', 'test', 'two']\n",
    "             ]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_bleu SAMPLE\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# 注意：在references中，每个参考句子都是一个列表的元素，即使只有一个参考句子\n",
    "references = [\n",
    "                [['this', 'is', 'test', 'one'], ['this', 'is', 'a', 'test', 'one']],\n",
    "                [['this', 'is', 'test', 'two'], ['this', 'is', 'a', 'test', 'two']]\n",
    "             ]\n",
    "candidates = [\n",
    "                ['this', 'is', 'test', 'one'],\n",
    "                ['this', 'is', 'test', 'two']\n",
    "             ]\n",
    "score = sentence_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29861622524500175\n"
     ]
    }
   ],
   "source": [
    "# DATA VIEW\n",
    "\n",
    "# tokenize \n",
    "refs_tok = [[wordpunct_tokenize(sentence)] for sentence in refs]\n",
    "tslts_tok = [wordpunct_tokenize(sentence) for sentence in tslts]\n",
    "\n",
    "score = corpus_bleu(refs_tok, tslts_tok)\n",
    "print(\"BLEU\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.]) tensor([1., 1.]) tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# sentence-lv bert_score sample\n",
    "from bert_score import score\n",
    "\n",
    "cands = ['this is 1', 'this is 2']\n",
    "refs = ['this is 1', 'this is 2']\n",
    "P, R, F1 = score(cands, refs, lang='en')\n",
    "print(P, R, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-lv avg_bert_score sample\n",
    "P, R, F1 = score(tslts, refs, lang='en', )\n",
    "avg_bert_score_P_R_F1=[P.mean().item(), R.mean().item(), F1.mean().item]\n",
    "print(\"avg_bert_score[P, R, F1]:\")\n",
    "avg_bert_score_P_R_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bleurt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7463241815567017\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from bleurt import score\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "checkpoint = \"bleurt/test_checkpoint\"\n",
    "references = [\"This is a test.\"]\n",
    "candidates = [\"This is the test.\"]\n",
    "\n",
    "scorer = score.BleurtScorer()\n",
    "scores = scorer.score(references=references, candidates=candidates)\n",
    "# assert isinstance(scores, list) and len(scores) == 1\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_scores = [scorer.score(references=[r], candidates=[t]) for r,t in zip(refs, tslts)]\n",
    "ls_scores = [i[0] for i in ls_scores]\n",
    "avg_bleurt_score = sum(ls_scores)/len(ls_scores)\n",
    "print(\"avg_bleurt_score:\",avg_bleurt_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 spt, get all evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is evaluation of ChatGPT-3.5 translation, where prompt is passing 50 sentences each time.\n",
      "\n",
      "Testing TP1 translation(BLEU):\n",
      "TP1 BLEU: 0.29861622524500175\n",
      "\n",
      "Testing TP1 translation(BERT SCORE):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1 avg_bert_score[P, R, F1]: [0.9404323101043701, 0.9407292604446411, 0.9405114650726318]\n",
      "\n",
      "Testing TP1 translation(Bleurt Score):\n",
      "TP1 avg_bleurt_score: 0.3482626240638395\n",
      "\n",
      "Testing TP2 translation(BLEU):\n",
      "TP2 BLEU: 0.2960195988951252\n",
      "\n",
      "Testing TP2 translation(BERT SCORE):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP2 avg_bert_score[P, R, F1]: [0.9402866363525391, 0.9401487112045288, 0.9401476979255676]\n",
      "\n",
      "Testing TP2 translation(Bleurt Score):\n",
      "TP2 avg_bleurt_score: 0.34887727711970606\n",
      "\n",
      "Testing TP3 translation(BLEU):\n",
      "TP3 BLEU: 0.28895951120580804\n",
      "\n",
      "Testing TP3 translation(BERT SCORE):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP3 avg_bert_score[P, R, F1]: [0.9387385845184326, 0.9389855861663818, 0.9387936592102051]\n",
      "\n",
      "Testing TP3 translation(Bleurt Score):\n",
      "TP3 avg_bleurt_score: 0.30952103921522695\n"
     ]
    }
   ],
   "source": [
    "# 50 spt, get all evaluations\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from bert_score import score as bert_score\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from bleurt import score as bleurt_score\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "print(\"This is evaluation of ChatGPT-3.5 translation, where prompt is passing 50 sentences each time.\")\n",
    "for i in range(1, 3+1):\n",
    "    ref_file = 'dataset/test.true.en'\n",
    "    tslt_file = 'translated/tp'+str(i)+'_tslt_en.txt'\n",
    "\n",
    "    refs = read_file(ref_file)\n",
    "    tslts = read_file(tslt_file)\n",
    "    # BLEU\n",
    "    print(\"\\nTesting TP\"+str(i)+\" translation(BLEU):\")\n",
    "    # tokenize \n",
    "    refs_tok = [[wordpunct_tokenize(sentence)] for sentence in refs]\n",
    "    tslts_tok = [wordpunct_tokenize(sentence) for sentence in tslts]\n",
    "\n",
    "    bleu_score = corpus_bleu(refs_tok, tslts_tok)\n",
    "    print(\"TP\"+str(i)+\" BLEU:\", bleu_score)\n",
    "\n",
    "    # BERT SCORE\n",
    "    print(\"\\nTesting TP\"+str(i)+\" translation(BERT SCORE):\")\n",
    "    P, R, F1 = bert_score(tslts, refs, lang='en')\n",
    "    avg_bert_score_P_R_F1=[P.mean().item(), R.mean().item(), F1.mean().item()]\n",
    "    print(\"TP\"+str(i)+\" avg_bert_score[P, R, F1]:\", avg_bert_score_P_R_F1)\n",
    "\n",
    "    # Bleurt Score\n",
    "    print(\"\\nTesting TP\"+str(i)+\" translation(Bleurt Score):\")\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "    scorer = bleurt_score.BleurtScorer()\n",
    "\n",
    "    ls_scores = [scorer.score(references=[r], candidates=[t]) for r,t in zip(refs, tslts)]\n",
    "    ls_scores = [i[0] for i in ls_scores]\n",
    "    avg_bleurt_score = sum(ls_scores)/len(ls_scores)\n",
    "    print(\"TP\"+str(i)+\" avg_bleurt_score:\", avg_bleurt_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 spt, get all evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program_Files_IDE\\anaconda3\\envs\\ev4nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is evaluation of ChatGPT-3.5 translation, where prompt is passing 1 sentence each time.\n",
      "\n",
      "Testing TP1 translation(BLEU):\n",
      "TP1 BLEU: 0.3089671865725252\n",
      "\n",
      "Testing TP1 translation(BERT SCORE):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1 avg_bert_score[P, R, F1]: [0.9439606666564941, 0.9445890188217163, 0.9442088007926941]\n",
      "\n",
      "Testing TP1 translation(Bleurt Score):\n",
      "TP1 avg_bleurt_score: 0.36860787082463503\n",
      "\n",
      "Testing TP2 translation(BLEU):\n",
      "TP2 BLEU: 0.3003483421904474\n",
      "\n",
      "Testing TP2 translation(BERT SCORE):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP2 avg_bert_score[P, R, F1]: [0.9433439373970032, 0.9430471062660217, 0.9431304931640625]\n",
      "\n",
      "Testing TP2 translation(Bleurt Score):\n",
      "TP2 avg_bleurt_score: 0.3619335639104247\n",
      "\n",
      "Testing TP3 translation(BLEU):\n",
      "TP3 BLEU: 0.3106092340997083\n",
      "\n",
      "Testing TP3 translation(BERT SCORE):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP3 avg_bert_score[P, R, F1]: [0.9442692995071411, 0.9449902176856995, 0.9445641040802002]\n",
      "\n",
      "Testing TP3 translation(Bleurt Score):\n",
      "TP3 avg_bleurt_score: 0.3694909633261462\n"
     ]
    }
   ],
   "source": [
    "# 1 spt, get all evaluations\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from bert_score import score as bert_score\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from bleurt import score as bleurt_score\n",
    "import logging\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "print(\"This is evaluation of ChatGPT-3.5 translation, where prompt is passing 1 sentence each time.\")\n",
    "for i in range(1, 3+1):\n",
    "    ref_file = 'dataset/test.true.en'\n",
    "    tslt_file = 'translated/tp'+str(i)+'_tslt_en.txt'\n",
    "\n",
    "    refs = read_file(ref_file)\n",
    "    tslts = read_file(tslt_file)\n",
    "    # BLEU\n",
    "    print(\"\\nTesting TP\"+str(i)+\" translation(BLEU):\")\n",
    "    # tokenize \n",
    "    refs_tok = [[wordpunct_tokenize(sentence)] for sentence in refs]\n",
    "    tslts_tok = [wordpunct_tokenize(sentence) for sentence in tslts]\n",
    "\n",
    "    bleu_score = corpus_bleu(refs_tok, tslts_tok)\n",
    "    print(\"TP\"+str(i)+\"corpus_bleu:\", bleu_score)\n",
    "\n",
    "    # BERT SCORE\n",
    "    print(\"\\nTesting TP\"+str(i)+\" translation(BERT SCORE):\")\n",
    "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)\n",
    "    P, R, F1 = bert_score(tslts, refs, lang='en')\n",
    "    avg_bert_score_P_R_F1=[P.mean().item(), R.mean().item(), F1.mean().item()]\n",
    "    print(\"TP\"+str(i)+\" avg_bert_score[P, R, F1]:\", avg_bert_score_P_R_F1)\n",
    "\n",
    "    # Bleurt Score\n",
    "    print(\"\\nTesting TP\"+str(i)+\" translation(Bleurt Score):\")\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "    scorer = bleurt_score.BleurtScorer()\n",
    "\n",
    "    ls_scores = [scorer.score(references=[r], candidates=[t]) for r,t in zip(refs, tslts)]\n",
    "    ls_scores = [i[0] for i in ls_scores]\n",
    "    avg_bleurt_score = sum(ls_scores)/len(ls_scores)\n",
    "    print(\"TP\"+str(i)+\" avg_bleurt_score:\",avg_bleurt_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev4symProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
